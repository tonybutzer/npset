{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Landcover routine\n",
    "    1) Reclassify existing landcover dataset(s)\n",
    "    2) Derive majority categories for parcels\n",
    "    3) Implement \"burn in\" routine to make crop parcels more discrete\n",
    "    4) Cleanup misclassified crop pixels\n",
    "    \n",
    "The routine outlined above can be acheived using GIS software. The following code shows a Python implementation to derive starting landcover for a region. This code largely supersedes existing landcover processing tools in the data_processing.tools.landcover module.\n",
    "\n",
    "*Author*: Jordan Dornbierer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import gdal\n",
    "import json\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "\n",
    "local_path_to_repos = 'd:/Data/repos/python'\n",
    "sys.path.insert(0, local_path_to_repos)\n",
    "from data_processing.lib import utils\n",
    "from data_processing.tools import landcover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Datasets\n",
    "    1) Dataset defining study area region and extents, typically region/background = 1/0.\n",
    "    2) CDL, NLCD, or other landcover dataset that will be reclassified.\n",
    "    3) Ownership parcels dataset. Each parcel should have a unique id.\n",
    "    \n",
    "**Note**: The landcover, parcel, and region datasets should conform in spatial projection, extent, and resolution. However, landcover and parcel datasets should not be masked for the region. This occurs during the burn-in routine and enables the retainment of edge-of-region parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working dir\n",
    "%cd z:/Working/Delaware_Basin/Starting_LULC_v2/\n",
    "# %cd z:/Working/Northern_Lakes_Forest/starting_LULC\n",
    "\n",
    "# Set input and output directorys\n",
    "# in_dir = 'z:/Working/Delaware_Basin/Starting_LULC_v2/input'\n",
    "# out_dir = 'z:/Working/Delaware_Basin/Starting_LULC_v2/output'\n",
    "# in_dir = './starting_LULC/input'\n",
    "# out_dir = './starting_LULC/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source CDL or NLCD\n",
    "\n",
    "Get CDL or NLCD for region.  \n",
    "\n",
    "Note: \n",
    "- \"The 1997-2013 CDLs were recoded and re-released in January 2014 to better represent pasture and grass-related categories. A new category named Grass/Pasture (code 176) collapses the following historical CDL categories: Pasture/Grass (code 62), Grassland Herbaceous (code 171), and Pasture/Hay (code 181). This was done to eliminate confusion among these similar land cover types which were not always classified definitionally consistent from state to state or year to year and frequently had poor classification accuracies. This follows the recoding of the entire CDL archive in January 2012 to better align the historical CDLs with the current product. For a detailed list of the category name and code changes, please visit the Frequently Asked Questions (FAQ's) section at <http://www.nass.usda.gov/Research_and_Science/Cropland/sarsfaqs2.php\"\n",
    "\n",
    "- https://www.nass.usda.gov/Research_and_Science/Cropland/docs/CDL_2013_crosswalk.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region name.\n",
    "# region_name = 'eco45iv'\n",
    "region_name = 'delaware_basin'\n",
    "# region_name = 'eco50iv'\n",
    "\n",
    "# Get dataset defining study area region.\n",
    "region_ds = gdal.Open('./input/{}_region.tif'.format(region_name))\n",
    "region_array = region_ds.ReadAsArray()\n",
    "\n",
    "year = 2008\n",
    "\n",
    "if os.path.exists('./output_{}'.format(year)) == False:\n",
    "    os.mkdir('./output_{}'.format(year))\n",
    "\n",
    "# Get original CDL for the study area.\n",
    "# Note: We won't mask to the actual region boundary until majority parcels have been derived\n",
    "#  so that crop fields are preserved in their entirety (not cut) at the border.\n",
    "\n",
    "# Regionally subset CDL\n",
    "region_cdl_fn = '{}_cdl{}.tif'.format(region_name, year)\n",
    "region_cdl_fp = './input/{}'.format(region_cdl_fn)\n",
    "\n",
    "# CONUS ref\n",
    "cdl_fp = 'z:/Working/National_Datasets/Cropland_Data_Layer/{0}_30m_cdls/{0}_30m_cdls.img'.format(year)\n",
    "cdl_ds = gdal.Open(cdl_fp)\n",
    "                      \n",
    "if os.path.exists(region_cdl_fp):\n",
    "    # Load from existing file.\n",
    "    cdl_array = gdal.Open(region_cdl_fp).ReadAsArray()\n",
    "else:\n",
    "    # Clip from original CONUS CDL.\n",
    "    cdl_array = utils.raster_to_array(cdl_ds, region_ds)\n",
    "    \n",
    "    # Write clipped CDL2018 to new raster if desired\n",
    "    utils.array_to_raster(cdl_array, region_cdl_fp, region_ds, build_rat=True, cmap_ref=cdl_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destination Color Schema\n",
    "This can be derived from an existing raster dataset with the desired class-value : color schema.  \n",
    "Or from a JSON file containing class-values mapped to colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From foresce/model/foresce_lib/geotiffs.py:\n",
    "# Convert color hexidecimal string of RGB values to base10 tuple\n",
    "def parse_color_tuple(color_data):\n",
    "    if isinstance(color_data, (list, tuple)):\n",
    "        return color_data\n",
    "\n",
    "    color_data = color_data.lstrip('#')\n",
    "    length = len(color_data)\n",
    "    return tuple(int(color_data[i:i + length // 3], base=16) for i in range(0, length, length // 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colormap can be provided by any existing dataset with same color schema\n",
    "# cmap_ds = gdal.Open('z:/working/Delaware_Basin/Starting_LULC/CDL_2018_reclass.tif')\n",
    "\n",
    "# Class names, values, colors\n",
    "region_schema_fp = './input/{}_schema_with_color.json'.format(region_name)\n",
    "with open(region_schema_fp, 'r') as fh:\n",
    "    region_schema = json.load(fh)\n",
    "\n",
    "# Create color table\n",
    "ct = gdal.ColorTable()\n",
    "for cls_val, cls_dict in region_schema.items(): #cmap_config['classes']:\n",
    "    color_tuple = parse_color_tuple(cls_dict['color'])\n",
    "    ct.SetColorEntry(int(cls_val), color_tuple)\n",
    "\n",
    "print(region_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reclassify\n",
    "Reclassify NLCD or CDL to get a landcover dataset in the desired classification schema.\n",
    "\n",
    "The routine below uses dictionary of key: value pairs to map old landcover values to new.\n",
    "\n",
    "The recalssification schema can be created in the Python console or Jupyter Notebook, or loaded from a JSON file or CSV.\n",
    "Here, we load from an existing JSON file.  \n",
    "\n",
    "We assess the application of the reclass schema and make adjustments as necessary before creating the reclassified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdl_ds.GetDescription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Raster Attribute Table (rat) from CDL.\n",
    "cdl_bnd = cdl_ds.GetRasterBand(1)\n",
    "cdl_rat = cdl_bnd.GetDefaultRAT()  # Raster Attribute Table\n",
    "cdl_rat_fields = {i: cdl_rat.GetNameOfCol(i) for i in range(cdl_rat.GetColumnCount())}\n",
    "print(cdl_rat_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names from appropriate column/field. See above. This can change depending on vintage.\n",
    "# Note: these are indexed to class value i.e. class_names[class_val] returns the value's corresponding name.\n",
    "cdl_class_names = cdl_rat.ReadAsArray(4)\n",
    "\n",
    "# We can examine the from-value, name, and pixel count.\n",
    "# Note: categories with high pixel counts might be considered for their own class in the new schema.\n",
    "# print(list(zip(remaining_from_vals, cdl2018_class_names[remaining_from_vals], remaining_counts)))\n",
    "\n",
    "# Examine CDL class counts for the region.\n",
    "cdl_vals, cdl_counts = np.unique(cdl_array[region_array!=0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdl_df = pd.DataFrame(index=cdl_vals)\n",
    "cdl_df['names'] = cdl_class_names[cdl_vals]\n",
    "cdl_df['counts'] = cdl_counts\n",
    "cdl_df['perc'] = np.round(cdl_counts/(region_array!=0).sum(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine highest count categories.\n",
    "sort_index = cdl_df['counts'].sort_values(ascending=False).index\n",
    "cdl_df.loc[sort_index].to_csv('./input/{}_cdl{}_counts.csv'.format(region_name, year))\n",
    "cdl_df.loc[sort_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each key in the recode/reclass dictionary is a single to-class value.\n",
    "# The key's item is a list of one or more from-class values.\n",
    "reclass_schema_fp = './input/{}_cdl_recode.json'.format(region_name)\n",
    "with open(reclass_schema_fp, 'r') as fh:\n",
    "    reclass_schema = json.load(fh)\n",
    "\n",
    "print(reclass_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note, above, that to-class 8 will catch all remaining from_class values that are not explicitly assigned a new class.\n",
    "# Check to ensure these categories are all crops before finishing the reclassification.\n",
    "# Edit reclassification JSON as necessary.\n",
    "\n",
    "# Assemble all recode from-vals.\n",
    "recode_vals = []\n",
    "for from_vals in reclass_schema.values():\n",
    "    if isinstance(from_vals, list):\n",
    "        recode_vals.extend(from_vals)\n",
    "        \n",
    "catch_all = [val for val in cdl_vals if val not in recode_vals]\n",
    "cdl_df.loc[catch_all].sort_values('counts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute counts for reclassified.\n",
    "\n",
    "# Initialize reclassified DataFrame\n",
    "reclass_vals = [int(val) for val in reclass_schema]\n",
    "reclass_df = pd.DataFrame(index=reclass_vals)\n",
    "\n",
    "# Initialize columns\n",
    "reclass_df['names'] = ''\n",
    "reclass_df['counts'] = 0\n",
    "\n",
    "for val in reclass_df.index:\n",
    "    reclass_df.loc[val, 'names'] = region_schema[str(val)]['name']\n",
    "    from_vals = reclass_schema[str(val)]\n",
    "    \n",
    "    # Account for catch-all category\n",
    "    if isinstance(from_vals, list) == False:\n",
    "        from_vals = catch_all\n",
    "        \n",
    "    # Sum from-val counts for all from-classes going to given to-class    \n",
    "    mask = np.in1d(cdl_vals, from_vals)\n",
    "    reclass_df.loc[val, 'counts'] = cdl_counts[mask].sum()\n",
    "\n",
    "# Compute %\n",
    "reclass_df['perc'] = np.round(reclass_df['counts']/(region_array!=0).sum(), 4)\n",
    "\n",
    "# Examine reclassified counts. Edit schema if necessary and recompute.\n",
    "reclass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reclassified dataset\n",
    "\n",
    "reclass_fn = region_cdl_fn[:-4] + '_reclass.tif'\n",
    "reclass_fp = './output_{}/{}'.format(year, reclass_fn)\n",
    "\n",
    "if os.path.exists(reclass_fp):\n",
    "    reclass_array = gdal.Open(reclass_fp).ReadAsArray()\n",
    "    \n",
    "else:\n",
    "\n",
    "    # initialize reclassified landcover\n",
    "    reclass_array = np.zeros(cdl_array.shape, dtype=np.uint8)\n",
    "\n",
    "    # Reclassify all from-vals mapped to to-vals as defined in the reclassify schema\n",
    "    for to_val, from_vals in reclass_schema.items():\n",
    "        if isinstance(from_vals, list):\n",
    "            for from_val in from_vals:\n",
    "                reclass_array[cdl_array == from_val] = int(to_val)\n",
    "\n",
    "    # If all the remaining classes can sensibly be converted to class 8 \"Other Crops\" we can proceed.\n",
    "    # Note: do not convert 0: \"Background\"\n",
    "    remaining_mask = (reclass_array == 0) & (cdl_array != 0)\n",
    "    reclass_array[remaining_mask] = 8\n",
    "\n",
    "    # Write to raster dataset if desired.\n",
    "    utils.array_to_raster(reclass_array, reclass_fp, region_ds, build_rat=True, cmap_ref=ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Majority Category Parcels\n",
    "Use existing parcels dataset and the reclassified landcover dataset to derive majority categories at the parcel level for the region.\n",
    "\n",
    "The steps below preserve parcels at the edge of the region.\n",
    "\n",
    "Note: previously multiple applications (x4) of Majority Filter function in ArcMap (neighbors=4, replacement_thresh='half') was used to clean up parcels. However, has been found to eliminate small parcels, which is problematic for urban decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D index of unique parcels using scipy's ndimage module to iterate through parcels.\n",
    "\n",
    "# Note: each item in the index is a tuple of slices bounding the 2D parcels\n",
    "# Each tuple's positional index, i, corresponds to the unique parcel id i + 1\n",
    "# E.g. index[123] returns the tuple for parcel with id = 124.\n",
    "\n",
    "# First make parcel NoData = 0, as the find_objects() function expects background = 0.\n",
    "parcels_array[parcels_array == parcel_nodata] = 0\n",
    "index = np.array(ndimage.find_objects(parcels_array))  # list -> array for fancy indexing below\n",
    "\n",
    "# Get unique parcel ids for the region and remove nodata = 0 from ids.\n",
    "region_ids = np.unique(parcels_array[region_array != 0])\n",
    "if 0 in region_ids:\n",
    "    region_ids = region_ids[region_ids != 0]\n",
    "\n",
    "# Create dictionary with parcel id: bounding box slices pairs for only the given region,\n",
    "# excluding parcels outside the region.\n",
    "# Note: using numpy fancy indexing much more efficinet than checking each parcel id for membership in region_ids.\n",
    "region_index = dict(zip(region_ids, index[region_ids - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or supply list of crop class values\n",
    "crop_vals = [1, 2, 3, 4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update 3/10/20\n",
    "Prioritizing crops in majority parcels workflow.  \n",
    "Current minimum criteria for parcel to be assigned to crop is 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset of parcels for region.\n",
    "# parcels_filename = 'delaware_basin_clu_hsin_parcels_maj_filt_x4.tif'\n",
    "# parcels_filename = 'delaware_basin_clu_yanroy_hsin_parcels_majfilt_x4.img'  # TODO: BigTiff issue\n",
    "# parcels_filepath = os.path.join(in_dir, parcels_filename)\n",
    "\n",
    "parcels_filepath = './input/delaware_basin_clu_hsin_parcels.tif'\n",
    "parcels_ds = gdal.Open(parcels_filepath)\n",
    "\n",
    "# Here, Arc's Majority Filter function (# neighbors = 4, replacement thresh = half) was run 4x to clean up parcel edges.\n",
    "# This is optional.\n",
    "\n",
    "# Get parcels nodata value and array.\n",
    "parcel_nodata = parcels_ds.GetRasterBand(1).GetNoDataValue()\n",
    "parcels_array = parcels_ds.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_cat_fn = '{}_{}_majority_categories.tif'.format(region_name, year)\n",
    "maj_cat_fp = './output_{}/{}'.format(year, maj_cat_fn)\n",
    "\n",
    "if os.path.exists(maj_cat_fp):\n",
    "    maj_array = gdal.Open(maj_cat_fp).ReadAsArray()\n",
    "\n",
    "else:\n",
    "    # Initialize parcel majority dataset\n",
    "    maj_array = np.zeros(cdl_array.shape, dtype=np.uint8)\n",
    "\n",
    "    # Iterate through each parcel of the region. Using bounding box (bb) to isolate parcel pixels.\n",
    "    for parcel_id, bb in region_index.items():\n",
    "\n",
    "        if bb is None:\n",
    "            continue\n",
    "        \n",
    "        # Initialize\n",
    "        maj_found = False\n",
    "\n",
    "        # Convert to tuple\n",
    "        bb = tuple(bb)\n",
    "\n",
    "        parcel_box = parcels_array[bb]\n",
    "\n",
    "        # Isolate parcel pixels inside bounding box\n",
    "        bb_mask = parcel_box == parcel_id\n",
    "\n",
    "        # Optionally apply size criteria.\n",
    "        if bb_mask.sum() > 2878:  # 2878 30m pixels is ~1 section\n",
    "            continue\n",
    "\n",
    "        # Find all category values for parcel\n",
    "        parcel_vals = reclass_array[bb][bb_mask]  # 2D -> 1D\n",
    "        unique, counts = np.unique(parcel_vals, return_counts=True)\n",
    "                \n",
    "        # Find crop category values for parcel\n",
    "        crop_idx = np.in1d(unique, crop_vals)\n",
    "        \n",
    "        # If parcel constains crops attempt to prioritize crop categories over others\n",
    "        if crop_idx.sum() > 0:\n",
    "            unique_crop = unique[crop_idx]\n",
    "            counts_crop = counts[crop_idx]\n",
    "            if counts_crop.max()/parcel_vals.size >= .25:\n",
    "                # Set unique, counts to crops\n",
    "                unique = unique_crop\n",
    "                counts = counts_crop\n",
    "\n",
    "        # Determine max val for parcel, account for potential tying values\n",
    "        max_vals = unique[counts == counts.max()]\n",
    "        max_val = np.random.choice(max_vals)\n",
    "\n",
    "        # Set majority value in maj_array\n",
    "        maj_array[bb][bb_mask] = max_val\n",
    "\n",
    "    # Write majority category parcels raster dataset if desired.\n",
    "    utils.array_to_raster(maj_array, maj_cat_fp, region_ds, build_rat=True, cmap_ref=ct, nodata=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Apply burn-in routine\n",
    "Here the burn-in schema is saved in a JSON file.\n",
    "\n",
    "Values found under the \"field_vals\" are crop category values from the reclassified landcover dataset.\n",
    "\n",
    "Values under \"burn_all\" define categories that should be burned back in everywhere, unaltered from the reclassified landcover dataset.\n",
    "\n",
    "The \"burn_some\" key corresponds with a nested dictionary of to-val: from-vals. This enables burning in to be controlled at the class level if desired. For each to-class all classes in the from-vals are allowed to burn in.  \n",
    "\n",
    "For Delaware Basin no \"burn_some\" values are specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the burn-in schema from JSON\n",
    "\n",
    "burn_schema_fp = './input/delaware_basin_burn_schema.json'\n",
    "with open(burn_schema_fp, 'r') as fh:\n",
    "    burn_schema = json.load(fh)\n",
    "\n",
    "print(burn_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_fn = reclass_fn[:-4] + '_maj_cat_burn_schema.tif'\n",
    "burned_fp = './output_{}/{}'.format(year, burned_fn)\n",
    "\n",
    "if os.path.exists(burned_fp):\n",
    "    burned_array = gdal.Open(burned_fp).ReadAsArray()\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Initialize burned-in array by copying the reclassified landcover\n",
    "    burned_array = reclass_array.copy()\n",
    "\n",
    "    # Burn in region boundary\n",
    "    burned_array[region_array == 0] = 0\n",
    "\n",
    "    # Burn in majority parcels for \"field_vals\"\n",
    "    # Note: this extends parcels beyond region boundary if necessary.\n",
    "    for field_val in burn_schema['field_vals']:\n",
    "        burned_array[maj_array == field_val] = field_val\n",
    "\n",
    "    # Burn from-to specified by \"burn_some\"\n",
    "    for from_val, to_vals in burn_schema['burn_some'].items():\n",
    "        from_mask = burned_array == from_val\n",
    "        for to_val in to_vals:\n",
    "            to_mask = reclass_array == to_val\n",
    "            burned_array[from_mask & to_mask] = to_val\n",
    "\n",
    "    # Burn to-classes from \"burn_all\"\n",
    "    from_mask = burned_array != 0\n",
    "    for to_val in burn_schema['burn_all']:\n",
    "        to_mask = reclass_array == to_val\n",
    "        burned_array[from_mask & to_mask] = to_val\n",
    "    \n",
    "    # Write to raster\n",
    "    utils.array_to_raster(burned_array, burned_fp, region_ds, build_rat=True, cmap_ref=ct, nodata=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Clean up aberrant crop pixels\n",
    "Identify pixels that are classified among crops in the new landcover dataset but overlay parcels with majority categories that are not crop.\n",
    "\n",
    "These pixels can be converted to the underlying majority parcel type or to the nearest natural vegetation class.\n",
    "\n",
    "**Note**: The nearest dataset can be produced from any landcover dataset and a list of class values. However, the generation of nearest datasets using Euclidean distance may cause MemoryErrors for large regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine misclassified pixels before correcting\n",
    "misclass_fp = './output_{0}/{1}_{0}_misclass_crop.tif'.format(year, region_name)\n",
    "\n",
    "if os.path.exists(misclass_fp):\n",
    "    misclass_array = gdal.Open(misclass_fp).ReadAsArray()\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Initialize\n",
    "    misclassified = np.zeros(burned_array.shape, np.uint8)\n",
    "\n",
    "    # Majority categories NoData mask\n",
    "    maj_nodata_mask = maj_array == 0\n",
    "\n",
    "    # Isolate crop pixels outside their crop-majority parcels (where parcels exist)\n",
    "    for crop_val in crop_vals:\n",
    "        maj_mask = maj_array == crop_val\n",
    "        burned_mask = burned_array == crop_val\n",
    "        misclass_mask = burned_mask & ~maj_mask & ~maj_nodata_mask\n",
    "        misclassified[misclass_mask] = crop_val\n",
    "        \n",
    "    # Write to raster\n",
    "    utils.array_to_raster(misclassified.astype(np.uint8), misclass_fp, region_ds, cmap_ref=ct, build_rat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine misclassified pixels before correcting\n",
    "miss_vals, miss_counts = np.unique(burned_array[misclassified!=0], return_counts=True)\n",
    "all_vals, all_counts = np.unique(burned_array, return_counts=True)\n",
    "miss_idx = np.in1d(all_vals, miss_vals)\n",
    "\n",
    "# Populate Pandas Dataframe\n",
    "df = pd.DataFrame(index=miss_vals, columns=['name', 'misclassified', 'total', 'perc'])\n",
    "df.misclassified = miss_counts\n",
    "df.total = all_counts[miss_idx]\n",
    "df.perc = df.misclassified / df.total * 100\n",
    "\n",
    "# Add names to 'name' column\n",
    "for val in region_schema:\n",
    "    if int(val) in df.index:\n",
    "        df.loc[int(val), 'name'] = region_schema[val]['name']\n",
    "\n",
    "# Show the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_fn = burned_fn[:-4] + '_cleanup.tif'\n",
    "cleanup_fp = './output_{}/{}'.format(year, cleanup_fn)\n",
    "\n",
    "if os.path.exists(cleanup_fp):\n",
    "    cleanup_array = gdal.Open(cleanup_fp).ReadAsArray()\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Initialize cleanup array by copying burn-in array.\n",
    "    cleanup_array = burned_array.copy()\n",
    "\n",
    "    # Proceed to convert misclassified crop pixels to nat veg nearest or majority parcel class if it makes sense to do so.\n",
    "    # cleanup_array[misclassified_mask] = nv_nearest_array[misclassified_mask]\n",
    "    cleanup_array[misclassified != 0] = maj_array[misclassified != 0]\n",
    "\n",
    "    # Write to raster\n",
    "    utils.array_to_raster(cleanup_array, cleanup_fp, region_ds, build_rat=True, cmap_ref=ct, nodata=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternatively could assign misclassified pixels to nat-veg nearest.\n",
    "\n",
    "# # Get nat veg classes from reclassification schema.\n",
    "# nv_vals = [12, 13, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "# # Load existing or derive natural vegetation nearest dataset.\n",
    "# nv_nearest_fn = reclass_fn[:-4] + '_nv_nearest.tif'\n",
    "# nv_nearest_fp = './output/{}'.format(nv_nearest_fn)\n",
    "\n",
    "# if os.path.exists(nv_nearest_fp):\n",
    "#     nv_nearest_array = gdal.Open(nv_nearest_fp).ReadAsArray()\n",
    "# else:\n",
    "#     nv_nearest_array = landcover.get_nearest(reclass_array, nv_vals)\n",
    "    \n",
    "#     # Write to file to avoid recalculating.\n",
    "#     utils.array_to_raster(nv_nearest_array, nv_nearest_fp, region_ds, cmap_ref=ct, nodata=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute counts for reclassified and cleaned-up\n",
    "# Note: masking w/ cleanup_array retains whole fields at boundary\n",
    "# reclass_array = gdal.Open('./output/delaware_basin_cdl2018_reclass.tif').ReadAsArray()\n",
    "# cleaned_array = gdal.Open('./output/delaware_basin_cdl2018_reclass_maj_cat_burn_schema_cleanup.tif').ReadAsArray()\n",
    "\n",
    "mask = cleanup_array != 0\n",
    "unique_rcls, counts_rcls = np.unique(reclass_array[mask], return_counts=True)\n",
    "unique_clnd, counts_clnd = np.unique(cleanup_array[mask], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reclass_vals)\n",
    "print(ur)\n",
    "print(cr)\n",
    "print(uc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame with reclassified index\n",
    "change_df = pd.DataFrame(index=reclass_vals)\n",
    "\n",
    "# Add class names using region schema\n",
    "change_df['names'] = [region_schema[str(val)]['name'] for val in reclass_vals]\n",
    "\n",
    "# Add reclassified counts\n",
    "idx_mask = np.in1d(reclass_vals, unique_rcls)\n",
    "count_mask = np.in1d(unique_rcls, reclass_vals)\n",
    "change_df['reclass'] = 0  # initialize\n",
    "change_df.loc[idx_mask, 'reclass'] = counts_rcls[count_mask]\n",
    "\n",
    "# Add burned-in/cleaned-up counts\n",
    "idx_mask = np.in1d(reclass_vals, unique_clnd)\n",
    "count_mask = np.in1d(unique_clnd, reclass_vals)\n",
    "change_df['cleaned'] = 0\n",
    "change_df.loc[idx_mask, 'cleaned'] = counts_clnd[count_mask]\n",
    "\n",
    "# Comparison metrics\n",
    "change_df['diff'] = change_df['cleaned'] - change_df['reclass']\n",
    "change_df['perc_change'] = change_df['diff']/change_df['reclass'] * 100\n",
    "\n",
    "# Write to csv\n",
    "change_df.to_csv('./output_{0}/{1}_{0}_reclass_and_cleaned_histograms.csv'.format(year, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdl_ds.GetDescription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
